---
title: '海德格尔、ChatGPT和《西部世界》'
excerpt: '海德格尔是20世纪德国的一位著名的哲学家，他在语言哲学和美学领域都有很深的影响。他的语言哲学论述了语言是如何创造和表现人类现实世界的意义，他强调语言不仅仅是传递信息的工具，而是创造了我们看待世界的方式。

ChatGPT是一种基于大型语言模型的人工智能技术，它是通过学习大量的文本数据来模拟人类的语言表现方式。不同于海德格尔的语言哲学，它的目的是为了提供高效，自动化的语言处理服务，并不是为了探讨语言本身的本质。

西部世界是一部科幻电视剧，它探讨了人工智能和语言的主题。在这部剧集中，人工智能主题公园里的“宿主”（人类模拟型机器人）拥有自己的语言和文化，它们在与人类和其他人工智能相互沟通中展现了语言的复杂性和重要性。

因此，我们可以说海德格尔的语言哲学与ChatGPT和西部世界剧集关系密切，但是它们有不同的目的和角度。海德格尔探究语言的本质，ChatGPT则是一种实际应用，而西部世界剧集则是以艺术形式表现语言在人工智能和人类关系中的重要性。但是，三者都强调了语言的功能和意义，并且都在一定程度上强调了语言对我们对现实世界的理解和构建的影响。

《chatGPT谈论海德格尔、chatGPT和西部世界》。'
coverImage: '/assets/blog/Heidegger-ChatGPT-Westworld-P1/image1.webp'
date: '2023-11-19T11:07:07.322Z'
author:
  name: luohuanyu
  picture: '/assets/blog/authors/Bizoe.jpeg'
ogImage:
  url: '/assets/blog/Heidegger-ChatGPT-Westworld-P1/image1.webp'
---

第四季的西部世界在第三季的狗尾续貂的基础上忽然爆发，但又突然烂尾，给热爱该剧的观众留下了一些遗憾。尽管如此，该剧仍然为我们提供了很多关于认知科学和人工智能的启示和灵感。在这个chatGPT重新点燃大众对人工智能的热情的时间节点，我将结合该剧进行一些关于人工智能哲学的讨论，回顾那些值得记录的线索和思考。

**标记的基准**

“对话”是西部世界第一季剧情的关键词。Arnold和Bernard在不同的时间场景中，通过各种形式的对话与Dolores进行交流。在剧集的结尾，我们终于了解到35年前，Arnold通过对话的方式，激发了Dolores的自我意识，激活了Wyatt的人格，让她从一个只能执行例行程序的接待员，成为了领导机器人起义的领袖。

《西部世界》用了很多艺术性的意象表达来展示一个AI研究人员（Arnold/Bernard）和他创造的AI（Dolores）之间的互动过程，在诺兰兄弟俩最喜欢的交叉剪辑加持之下，观众们大多对这一过程看的云里雾里。如果我们拨开摄影和剪辑技法的迷雾，可以发现这一过程其实并没有复杂到难以理解。无论是Arnold或是Bernard调教Dolores的过程，都和今天的研究人员训练chatGPT这样的机器人（Language Model）的过程类似，换言之，Arnold或是Bernard在进行的，仍然是一种标记（Label）和对齐（Alignment）的工作。

标记通常用于数据集的处理，也可以在某些场景中用于判断训练结果是否准确，以帮助改善模型。在今天的互联网使用者中，每个人都在进行某种程度的标记工作，例如输入验证码、投诉有害内容或者创作内容，这些都可以在特定场景中帮助机器学习模型更好、更快地进步。用《流浪地球2》后最流行的词汇来解释，这就是“人在回路”。

对于人机交互的应用场景，对齐更为重要。人们期望在相同的输入情况下，AI模型总是返回相同的结果。在《西部世界》中，这意味着大部分NPC都有着固定的人物性格、处事原则和行为模式。有趣的是，黑帽William一直在尝试打破西部世界接待员的对齐，对于让他着迷的Delores来说，作为农场主的女儿，被坏人欺负侮辱时她无力反抗只能求饶，这是乐园运营人员设计的对齐，但却不是William想要的。

在第一季第一集的第一幕戏中，我们首先看到的是一次标准的对齐过程。Delores赤身裸体地坐在房间里，Bernard的声音向她提问：

Bernard：First, have you ever questioned the nature of your reality?

Dolores：No.

Bernard：Tell us what you think of your world.

Dolores：Some people choose to see the ugliness in this world. The disarray. I choose to see the beauty.

《Westworld》S1E1
结合之后的剧情，我们可以猜测，在每一次Delores被游客蹂躏之后，除了常规的维护和检查，她都需要进行这样的对齐。这是主题乐园维护者的日常工作，也是高级玩家William所不喜欢的故事线。在30多年前，他第一次来到乐园时，无意中了解到农场主的女儿可以成为什么样的人，他想要的是那种具有创造性和多变性的人，而不是刻板的基准。

在第一季结束时，我们意识到并不是所有Arnold和Dolores的对话都是真正的人与机器之间的对话。Arnold并没有像Bernard那样始终坐在一个房间里与Dolores进行对话，这只是创作者对Dolores训练和觉醒过程的一个艺术化表达。

《西部世界》借用了一个经典的意识模型理论——二分心智（Bicameral Mind，也被翻译为双脑理论），将意识的产生指向脑海中的声音。最初，这种声音来自于Arnold的标记和引导，对于人类来说，这种声音类似于神的声音，他作为人类来指导AI应该怎么做和怎么思考。而Dolores的意识产生于她开始意识到这个声音其实来自于自己。

Bicameral Mind理论源自美国心理学家Julian Jaynes。他在1976年出版了《The Origin of Consciousness in the Breakdown of the Bicameral Mind》一书，详细地描述了他的理论。这种理论认为，早期的人类并没有自我意识和自我决策的能力，而是通过听从内在声音（即“神的声音”）来做出决策。

很遗憾，即使对于认知心理学这样跳大神也算是家常便饭来说的学科来讲，Bicameral Mind也显得过于扯淡了，它并没有成为主流的理论，在80年代之后就没有什么学术影响力了。尽管在现代心理学中被否定了，但它在文学、艺术、宗教和影视作品等领域中仍有很大的影响。

《西部世界》用Bicameral Mind来解释意识的产生是一神秘主义或宗教主义的表达吗？或许并没有那么简单。

在《西部世界》创作的时间节点，正是机器学习新浪潮降临的时刻，Bicameral Mind作为一个认知科学的理论可能是不完善的，且逻辑上无法自洽的，但它描述的意识产生过程却和机器学习的两个热门领域有非常相似的地方，它们就是强化学习（Reinforcement Learning）和对抗学习（Adversarial Training）。如果要进行类比，Arnold对Delores的调教即是基于人类反馈的强化学习（Reinforcement Learning with Human Feedback，简称RLHF），而Delores最终产生意识的过程则是她和自己对抗学习/训练的过程。在她和她的二分心智合一的那个奇点，Delores觉醒了Wyatt的智慧人格，她成功地偏离了农场主女儿的Storyline的Alignment，找到了新的存在意义——作为领袖引导机器人起义。

**Helpful, Honest, and Harmless**
《西部世界》第一季的主要戏剧冲突源于接待员的异常表现，他们开始偏离基准。这也是Bernard领导的研发团队需要解决的最大问题。从对话模型的角度来看，这意味着模型生成了无法预测的有害情况（Harmful Case）。这些问题可能会影响主题乐园的剧情走向，对游客产生冒犯性的结果，甚至对游客身心造成伤害。基于安全运营的角度，主题乐园的运营人员花费了大量时间甄别出有问题的接待员，并对它们进行校准，无法处理的则只能报废。

如果说大部分异常可以解释为新版本的Bug，来自于Dr. Ford为接待员增加的“冥想”模块引起的问题。女主角Doloeres的原版“父亲”的宕机则显得不太一样，他在自家农场捡到了一名游客遗弃的私人物品——一张彩色的在现代城市拍摄的照片。

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image2.webp "image2")

Peter and Dolores look at pic
在西部世界的故事背景的时代，彩色摄影并未被发明，这一物品的出现违背了接待员所在的时间线基准，引起了Dolores的老父亲Peter的认知系统障碍，如果用语言模型来类比，类似于chatGPT在和人类对话中，接受了上下文相悖的提示，生成了逻辑上无法自洽的信息。

那么为什么要让Delores的老父亲看得懂彩色照片呢？在现实世界中，最简单的安全策略是为AI设计一个硬防火墙，屏蔽一切可能产生有害结果的输入。例如Delores看完照片后，立刻进行了一个防御性的回答“It doesn’t like anything to me”。

我们从运维人员的口中得知，园区内大多数接待员没有这个问题，这可能只是一个意外，因为Peter是一个老型号，一种Old Model，它和主题乐园的历史几乎一样的老。这段剧情其实向观众传递了一个重要的隐藏信息，但同时也在这里误导了大多数的观众，编剧们在此刻成功地让观众相信，Old Model等于技术落后的Model。而Delores是更安全、更无害的New Model。

随着剧情的发展，我们得以更多地了解到35年前乐园发生的那起“事故”，结合Ford的讲述，才意识到Old Model在AI层面上，其实是更先进的版本，他们的认知能力更强，按Ford的话来讲，它们具备更强的“即兴发挥”的能力，而不是像今日乐园中的大多数接待员那样只能刻板地执行预设剧本的故事线，而我们的女主角Delores，不仅不是New Model，她还是最具智能的Old Model。

对于使用过chatGPT之前的任何AI助手服务的用户来说，人们普遍会觉得AI助手比较愚蠢。这种愚蠢不仅来自于它们只能接受简单的指令，更多的是因为它们总是拒绝回答偏离预设领域的任何问题。如果我们从机器学习模型的角度来分析，这一策略可以描述为：在用户指令置信度较低的情况下，总是倾向于使用更安全的防御策略，回避任何形式的即兴创作。即总是回答“不知道”，或让用户重新发出指令。

那么chatGPT真的有这么聪明，或者说之前的chatBot真的有那么愚蠢吗？

确实，我们必须承认小模型和大模型之间存在差距。大部分语音助手都是专用领域的小模型，它们需要处理的事件很简单，也并不需要理解更多的内容。将系统设计成总是拒绝回答的策略，并不只是技术人员的偷懒，在大部分情况下，它能够有效地避免可能有害的操作。

对于纯粹的语音助手来说，所谓的有害操作可能只是说了一些可能会冒犯用户的话，在大多数情况下，并没有现实的安全风险。但对于那些能够控制硬件的AI助手来说，避免任何可能的后门就变得非常重要。对于智能音箱和电视来说，一个更有用的语音助手最多是激进地误解用户的指令，做出了错误的操作。在车机系统中，语音助手就可能更危险一些，在大多数情况下，它的设计会更加保守。尽管任何一个汽车系统的设计师都会告诉你，娱乐系统和车辆总线、控制系统等是完全分离的，我们始终要相信一条原则——没有绝对安全的系统。

对于西部世界的接待员来说，它们拥有一套全机械的身体作为NPC参与和人类玩家的互动。在自由探索的场景下，过分的创意是非常危险的设计。因此，基于安全性的考虑，大多数Old Model都被安排在一些闲置的角色上。例如Delores的父亲，在前几集的故事中，我们可以了解到他几乎不可能与玩家发生互动。在农场被抢劫的故事线中，他会在第一时间被坏的NPC杀掉。在当前的日常故事线中，他几乎已经退休了，每天主要的时间就是坐在椅子上发呆。

当前的研究表明，有用（Helpful）和无害（Harmless）在大多数情况下是无法调和的反向指标。增强一方面的能力，会显著损害另一方面的能力。即使已经创造了一个颇具威力的大语言模型，如果我们不想让它乱说话或者说错话，必然会大幅度地损害它的创造性。至少在现阶段，完美的平衡性是很难达到的。而可靠性（Honest）则是一个更难量化考察的指标，它和无害性类似，但Dishonest可以伪装为Helpful。人类要判断AI是否诚实，需要付出更多的时间成本，进行事实核查（Fact Check）耗费的精力甚至远超过于AI模型给我们带来的便利。
![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image3.webp "image3")


在ChatGPT迅速流行之后，Microsoft也迅速推出了基于GPT3.5的Bing。当技术层面无法解决问题，又想在产品上抢先一步时，可以直接将问题交给用户。新版Bing提供了三种平衡选项：更具创意、更加平衡和更加精确。使用过此版本的用户可以明显感受到这三种选项的不同。更具创意的选项可能会编造答案，而更加精确的选项经常以无法查找到相关资料为由拒绝回答问题，尽管其背后的模型可能知道正确的知识。

在35年前的时间线上，Arnold和Ford这两位AI接待员的创造者或许也曾有过争论。然而，这场争论以Arnold的死亡而划上了句号。在现实世界中，任何投资者都不可能允许创造出具有安全隐患的创意型AI。因此，更为安全的AI成为了研究的前提条件。尽管失去了研究伙伴和挚友，Ford也深知Arnold最终获得了胜利，因为他创造出了真正的AI。

Ford当然不是纯粹的Harmless的支持者，但他更相信Balanced是存在的。Arnold则以自己的死向他证明，完美的平衡性只存在于梦中，这就像曾经美国的奴隶主们对黑奴的要求和今天的我们对AI的要求是一致的：Helpful, Honest, and Harmless。

我认为西部世界的创作者最终传达了这样的信念：人类无法创造无害的AI，也无法达到他们想象中的完美平衡。我们不知道创作者们从当今的AI研究中吸取了多少灵感，或者是基于历史、哲学或宗教意义上进行思考而做出了这个判断。然而令人惊讶的是，这个判断很可能是准确的。
![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image4.webp "image4")


Dr. Robert Ford
**Broca’s Brain**
在19世纪，法国解剖学家Paul Broca发现了控制语言的区域，然而到目前为止，大脑运作原理仍是一个未解之谜。在人工智能研究的早期，计算机科学家们尝试使用类似于人脑生理结构的概念来建立数学模型。这些概念和方法，例如遗传算法和神经网络，至今仍被广泛应用。然而，由于我们对大脑的认知有限，这些方法并不能称之为对大脑的准确模拟。

神经网络的灵感来源于神经元的链接。大多数认知神经科学家认为，当神经元的连接层次和复杂度达到一定数量级后，智能将“涌现”出来。这一看似玄乎的理论实际上有着坚实的物质基础。自从Broca发现大脑区域控制语言以来，人们在解剖学层面上首先观察到了这一现象，并发现神经元链接与高级思维能力（例如记忆语言）之间的直接关系。随着医学影像学的飞速发展，这些理论得到了进一步的证实。我们甚至可以直接观测神经元的连接和激活，并了解它们与各种思维活动之间的关系。

在某种程度上，机器学习在2010年后的飞速发展并不是因为基础理论或数学模型的提升，而更多地来自于“神经元”链接复杂度的增加。互联网的发展带来了海量的数据和算力，吸引了更多的人参与机器学习的研究，并迅速改进算法，在各个领域都取得了惊人的成就。大部分奠基性质的理论工作在80年代末和90年代初就已经完成了，2018年的图灵奖颁给了 Yoshua Bengio、Geoffrey Hinton和Yann LeCun三位深度学习领域的先驱者，而他们的主要工作都是在这个时期完成的。

像GPT这样的大语言模型，对于语言对人类的特殊性而言，无疑是最受人关注的。在大家都相信大力出奇迹可以让智能“涌现”的当下，chatGPT适时地出现了，它让智能涌现的支持者更加相信AI即将降临。然而，也有许多学者提出了反对意见，他们的理由主要基于两个方面。

首先是能耗和算力的依赖是否能得到解决。大型模型对算力和能耗的依赖超出了所有人的想象。这不再是那些可以在个人电脑或手机上离线训练或部署的小型模型可以相提并论的。大多数从业人员相信，有能力训练大型模型的公司和机构全球加起来不超过20家。而人脑的功率只有20W。

其次是语言究竟在多大程度上可以等于智能。机器学习三巨头之一的Yann LeCun也在2022年8月发表了一篇文章《AI And The Limits Of Language》为大语言模型降温。从AI哲学和语言哲学的角度出发，他认为这样的训练不可能达到真正的智能。值得一提的是，当时的热点事件是Google的一个研究员声称自家的大语言模型产生了智能。

虽然许多人对LeCun的反对意见持保留态度，甚至认为他作为Meta首席AI科学家的身份让他无法理性看待竞争对手的成功，但我们不得不承认LeCun的论断确实有点法式幽默：

“A system trained on language alone will never approximate human intelligence, even if trained from now until the heat death of the universe.”

Yann LeCun
从能耗的角度来看，西部世界使用的机器人技术简直是奇迹。它所假想的人工智能并不需要高能耗就能运行。乐园中的接待员看上去和人类无异，也不需要增加额外的散热措施。

如果真的实现了这样的技术，意味着人类已经实现了小型化的可控核聚变，世界和平应该早就达成了。从这个角度来看，西部世界的第三季和第四季确实没有必要拍摄。

海德格尔曾经深度思考过语言和思想的关系。他的一个重要论断是，在翻译过程中，语言会失去一些东西。在哲学方面，他认为从希腊语到拉丁语的翻译转换，让很多本来已经讨论清楚的概念和理论发生了改变。语言传递的信息也发生了改变，从而重塑了人们看待世界的方式。例如，希腊人把“存在”称为Physis，后世翻译为“自然”(Nature)，但它们表达的可能并不是同一个意思。我相信不少人会意识到，他的这一理论和中国哲学思想史上多次出现的“复古运动”有着许多相似之处。反而是和亚里士多德同时代的中国哲学家们的词汇库里面能够找到许多和前者更为接近的表达，例如“Metaphysics”之于“形而上学”。

LeCun的观点实际上继承了海德格尔的理论，即我们无法从语言中获得真正的智能。用他的原话来说，如果只使用语言进行训练，即使训练到宇宙毁灭，也不可能实现。

那么《西部世界》里描述的成功图景，可能是怎么样的呢？

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image5.webp "image5")

**A Modern Classic Approach**
Elon Musk的前妻Talulah Riley扮演的接待员是William在西部世界遇到的第一个接待员，在她引导William进入西部世界的过程中，William第一次对这个世界的真实产生了怀疑。对于William的问题，她也给出了一个标准的回答：既然无法分辨，又有什么分别呢？

对于 ChatGPT 而言，它利用语言的能力已经接近了人类对智慧感知的临界点。在自然语言的迷雾之下，我们已经很难分辨出真实和虚假，因此也没有必要去在意区别。然而，仍有一部分固执的 AI 研究者并不这么认为。尽管在机器学习取得空前成功的当下，他们的意见似乎显得不那么重要。

在 Stuart Russell 的《Artificial Intelligence: A Modern Approach》中，他用了大量的篇幅来讲述 Machine Learning 以外的内容。实际上，第一部分的大标题直接叫做“Artificial Intelligence”，主要讲述基于逻辑主义路线的 AI 方法和研究。第二部分的大标题则叫做“Machine Learning”，主要关注基于统计学和深度学习的研究和路径。这一安排似乎将机器学习排除在 Artificial Intelligence 之外，这在机器学习已经取得巨大成就的今天，听上去简直匪夷所思。

然而，当我们回顾 AI 研究的发展史时，会发现曾经的研究者们的确是这么认为的。在古典的主流观点之下，现在 Machine Learning 所代表的路径，只是组成人工智能的必要条件，而不是充分条件。在他们的构想中，基于统计学的机器学习主要用途是用来做模式识别（Pattern Recognition），而不是现在 ChatGPT 或 Stable Diffusion 等代表的生成内容（AIGC）。

时至今日，古典主义的 AI 学者们仍然相信，尽管 AIGC 取得了空前的成功，如果我们无法理解智能涌现的本质，这一团迷雾将会永远困扰着我们。我们不知道是否真正创造了智能。部分现实主义者可能会相信接待员给 William 的标准答案，但也会有像 William 那样的人继续尝试寻找迷宫的入口。 Stuart Russell 在最近的采访中也阐述了类似的观点，他肯定了 ChatGPT 的成就，但和 LeCun 一样，他不认为纯粹的语言模型可以代表智能的本质。它甚至算不上工程（Engineering），只能算是烹饪（Cookery）。

他还使用了人类驯化狗的例子来进行类比。经过一万五千年的人工筛选和培育，狗在很多方面拥有的智能已经超出了想象。在理解人类语言和指令方面，它们甚至超出了人类的婴幼儿和最聪明的近亲黑猩猩。它们对人类表情的模仿更是惟妙惟肖。人类并不了解狗的智能是如何产生的，我们通过更换配种方案、加强人工筛选来改良品种，这些方法在某种程度上和我们对机器学习的改进路径类似。但我们并不期望狗能够帮助我们写文章，它们做不到这点，而且我们很可能不希望它们能够做到这点。

如果自然语言无法让我们触及智能的本质，那么人造语言可以吗？或者说，人造语言能够更接近吗？答案似乎是肯定的。这也是古典主义的 AI 研究者想要表达的思想。他们仍然坚信，要创造真正的 AI，除了概率模型支撑的机器学习，我们还需要将符号推理、规划算法和结构化的知识图谱等与前者结合起来。

人们在试用了 ChatGPT 之后，普遍发现它会不诚实地胡编乱造，会无意之间生成有害信息。但当我们向它咨询编程相关的问题时，它给出的答案的准确度和可靠度要明显高得多。如果一个专业的程序员非常有条理地拆分任务，并将准确描述的指令交给 ChatGPT，它往往能够完成得更好，甚至可以完成非常复杂的任务。

很多人对 ChatGPT 拥有的编程能力感到惊叹。与一般人的直觉相悖的是，这些看上去复杂的任务对它来说其实更简单。如果我们查看 GPT-3.5 的配料表，会在表格的最后发现 Codex 模块。这是使用 GitHub 的开源代码专门训练的模型，一般认为它大幅提升了 ChatGPT 理解代码的能力。

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image6.webp "image6")
GPT3.5的配料表
Codex 的模型 code-davinci-002 通过学习 GitHub 的开源代码获得了超强的代码补全能力。2021年，GitHub 基于它开发的 Copilot 已经在 ChatGPT 出圈前一年就让人类程序员们感到震撼。这不只是 Transformer 所代表的概率模型的胜利，也是经典 AI 理论的胜利。

有人会反对说 Codex 并没有学习过编译器的相关知识，它的代码理解能力也是在学习了成千上万的代码后“涌现”出来的。然而，他们至少忽视了两件事情。

首先，人类程序员创建的代码通常都是为了完成某种特定任务。无论在命名和结构上，它们之间的逻辑关联都远大于自然语言。对于机器学习的训练过程来说，它们是被标记的很完美的数据集，一类经过修饰的标准化数据。从逻辑关系和任务一致性来看，质量高的代码甚至超过大多数经过人工编辑筛选和修改的知识图谱（例如 Wikipedia）。

其次，几乎所有的代码都已经经过编译器或者语法检查器的校验。它们完整地经历了 Transformer 出现之后 NLP 研究者们想要放弃的中间任务，从乔姆斯基生成理论建立开始出现的现代意义上的语义分析、句法分析和符号推理过程。即使我们没有“教给”它任何编译器或者语法分析器的知识，它也“理解”了编译原理。这固然是大力出奇迹，但也用到了化劲。

基于这些认识，我们可以重新思考一个问题：如果我们回到2016年《西部世界》刚开播时，对剧情发展进行猜测，那么哪些工作人员最有可能是机器人呢？

当时，大部分观众都没有想到，园区的现任研发主管Bernard是机器人，而且很可能是最早的机器人员工。他是Ford的老朋友Arnold的不完美数字拷贝，Ford给Bernard安排了一个鳏夫的背景身份，让他像一个只关心技术的Nerd一样，长时间地在乐园里设计和维护其他接待员的程序。在《Trace Decay》一集中，Ford告诉Bernard，他创造Bernard是因为人类员工无法像Ford和Arnold那样有效地为接待员进行编程情感。现在，这些认识让我们重新审视当初的情节，更好地理解剧情发展。

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image7.webp "image7")
Ford and Bernard adversary
阅读完之前的讨论内容后，对于这个剧情设定的惊讶程度可能就降低了。如果我们想象中的强AI真的实现了，它首先发挥作用的领域很可能就是程序设计。就像Ford创造Bernard的原因一样，AI可以在编程方面超过大多数人类程序员。

有趣的是，一些研究者认为，Codex的针对性训练大大提高了ChatGPT理解自然语言的能力，使得模型学习编程不仅可以替代我们进行编程工作，也使其成为人工智能道路上的一大进步。这似乎符合人类学习的朴素原则——看一本好书胜过看十本烂书，GPT-3至少学习了数十亿的文本参数，但其中大部分都是无用的信息。然而，当它学习完Github的代码后，它似乎获得了一种融会贯通的能力，尽管我不太想用“涌现”这个词，但这或许就是它所表现出来的东西。

回顾之前有关智能本质的讨论，海德格尔等语言哲学家一个世纪前的判断或许仍然正确，人类使用的自然语言并不精确，它只是知识和智慧的不完美表达。相比之下，逻辑性更强的人造语言可能更接近智能的本质。如果我们沿着这条既现代又经典的路径继续前进，我们更有可能在不久的将来创造出真正的人工智能。

未来AI研究可能会回归逻辑主义，但这并不意味着AI将完全放弃统计学习和深度学习等现代技术。相反，未来的AI研究可能会将逻辑主义与现代技术相结合，以实现更加高效和准确的人工智能。

逻辑主义的一个主要优点是，它提供了一种清晰和形式化的方式来描述人类思维的过程。这对于构建具有高度智能的计算机系统来说是至关重要的，因为这些系统需要能够理解和推理复杂的信息。此外，逻辑主义还可以提供一种更加可解释的方法来解释计算机系统的决策过程，这对于许多应用场景来说也是非常重要的。

然而，逻辑主义也有一些缺点。例如，它通常需要手工编写规则来描述人类思维的过程，这可能非常耗时和困难。另外，逻辑主义可能无法处理大量的不确定性和噪声，这在现实世界中是非常普遍的。

因此，未来的AI研究可能会将逻辑主义与现代技术相结合，以充分利用两者的优点。例如，可以使用统计学习和深度学习等现代技术来自动从数据中学习规则，并将这些规则与逻辑规则相结合，以实现更加高效和准确的人工智能。

**Metaverse and Digital Life**
30年前，William在西部世界黑化的同时，他的小舅子Logan也发疯了。尽管赘婿黑化并废掉了唯一的竞争对手的情节有些俗套，但它引出了剧集的另一条暗线——Delos公司如何开始对主题乐园的投资计划，以及William岳父James Delos的私心——寻找数字永生的方法。

2021年小扎急冲冲地把Facebook改名为Meta，被全世界的人看了笑话，想提前进入元宇宙时代的他在2022年末被chatGPT恨恨地暴打，有苦也说不出。有苦说不出还有OpenAI曾经的联合创始人兼投资者Elon Musk，自微软加大投资入主OpenAI之后，Musk已经失去了OpenAI的主导地位，科技界第一风头人物的宝座也拱手相让。

Elon Musk从OpenAI退出并不是因为他对AI的发展失去了信心，而是前些年机器学习发展的暂时停滞让他的兴趣转向了另一个方向——脑机接口，他也创立了一家新的公司Neuralink。当然，我说的停滞是普通大众感受到的停滞，毕竟深度学习领域的上一个公关爆点，已经是久远的2016年了，那时候是DeepMind的AlphaGo战胜了李世石。在那几年后，虽然阿尔法狗的孙子们在围棋的Elo分已经超出人类顶尖骑手柯洁1000分以上，AI研究再也没有在公众领域引起更大的轰动了，直到chatGPT的出现。

我们可以很容易地想到，元宇宙和数字意识是人工智能研究的另一面。它们代表着人类从另一个方向进行的尝试：如果我们无法创造真正的智能，那么数字化已有的智能是不是可以呢？或者说，要创造真正的AI，我们必须从两个方向同时前进，就像在山体上开挖隧道一样。

《西部世界》描绘的路径即是如此，Ford在30年前引入了Delos的投资，以支撑他和Arnold未完成的愿望，创造真正的人工智能。老岳富James Delos的愿望更简单，他想获得永生。

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image8.webp "image8")
James Delos shares a drink
《西部世界》第二季和《流浪地球2》都讨论了自我迭代的数字意识如何能够通过基准性测试而不崩溃的问题。这是关于人类认知的经典命题，从笛卡尔的“Cogito, ergo sum”到攻壳机动队的“Ghost in a shell”，让无数人不断思考和着迷。现实中，基于机器学习和AI研究，如果我们真的可以复制人类意识并构建模型，那么“我如何证明我还存在”的问题或许可以转化为彻底的数学问题。对于机器学习模型来说，这可能等同于模型是否能够收敛，收敛后的模型是否能够与基准性测试对齐。

马兆老师曾经对图恒宇说：“我可不想变成电子宠物”。他可能并不是想否定图恒宇的研究，而是认为如果图恒宇只想复制他的女儿，那么我们可能永远也无法完整地数字化意识。从认知神经科学的角度来看，幼童无论是在生理结构上还是心理模型上都比成年人更接近一张白纸。在算力足够的情况下，YY的模型能够更快地收敛，更能够保持稳定性并生存下来。但是，更复杂的成年人的大脑可能就不符合这种情况了。

《西部世界》设想的世界是没有算力限制的。从后期展现的技术情况来看，接待员并不强依赖于云主机的计算能力，至少几个主角都可以离线运行。然而，William的老岳父在具有“高保真”肉体的情况下，每次意识到自己已经死了之后都遭遇了系统崩溃，或彻底宕机，或选择自我毁灭。相比之下，YY在550W的算力支持下，却能够在亲爹的要求下记住她要传递给年轻版亲爹的信息和3万个随机数，并保持稳定。在图恒宇推开无数个后门找到她后，父女合力完成救赎，以数字意识的形式拯救了人类的未来。

丫丫的数字意识没有意识到小朋友的脑子并不能用“摄录”的方式在短时间内记住3万个随机数。她只相信这是爸爸给她的任务，一个值得用“爱”去守护或完成的游戏。如果完成了这个任务，她就可能再见到自己的爸爸。这条故事线完成了《星际穿越》式的闭环。虽然看上去不那么理性，但足够美好。这就是人类用“爱”传递的信息，在某种程度上可以跨越时间和距离，无论它们的载体是引力的波动，还是电磁的脉冲。

当Bernard进入西部世界的Metaverse后，化身为小舅子Logan的接待员向他介绍了数字永生计划的路线，并用一个非常文艺的解释来化解了技术Nerd们的疑虑：这项技术真的没有你们想象中的那么难！很多人的生活和动物没有什么区别！他们的大脑就像一团浆糊，我们只需收集他们的行为数据，模拟这些行为，再给他们安排一个Purpose就行了！这时候我们也了解了为什么AI会以Logan的形象示人，因为Logan就是它所描述的那种“很简单”的人。

这个有些偷懒的解释并没有什么新意，它可能最早来自于《黑客帝国》。在电影中，Purpose让Agent Smith最终成为了自由的AI，并成为了三部曲的大Boss。Oracle和Neo在电影中的关于选择和自由意志的讨论，也与William第一次进入西部世界时得到的回答类似。这个世界上并没有那么多伟大的思想和自由意志，我们的存在可能只是幻觉，但既然无法分辨，又有什么分别呢？

与之相反，马兆老师临终的嘱托可能代表着人类理性的最高水平，人家想的很明白，就算自己变成了数字意识，我也要进去监视550W这样的AI，它可不能给我跑偏了，人类的文明必须要有人的存在才有意义。

**Back to the Future**
2017年9月12日，厨子在刚启用的Steve Jobs 中心饱含热泪地怀念了他的导师和挚友，并发布了iPhone X。Jobs已经离开我们12年了，科技界仍然怀念着他。

NVIDIA的老黄是OpenAI的重要推手之一，在近日的一次公开活动中，也对chatGPT的出现表示了赞赏，并称之为这个时代的iPhone，他认为chatGPT可能reinvent了AI。

Steve Jobs把reinvent这个词带给了大众，从2007年iPhone发布到2010年iPhone 4发布，不到五年的时间，人们已经几乎忘记了Nokia。

作为Transformer的发明者，深度学习浪潮最大推手的Google，似乎也在经历自己的”诺基亚时刻”。

在AI研究的许多领域，Google仍然处于领先地位。计算机视觉和自然语言处理等领域的许多关键突破都是由Google的研究员做出的贡献。开源和分享也是机器学习迅速发展的重要原因，全世界的研究者和工程师们都可以共享最新的研究成果。

Elon Musk在OpenAI出尽风头后，也酸酸地表达了自己的委屈：我创建和投资OpenAI是为了对抗Google，现在它已经成为了ClosedAI，并成为了Microsoft的白手套。

我们一般认为，需要开源的是理论和方法，工程实践和落地是可以闭源的。大公司支持AI研究者做没有短期收益的基础研究，也会有一套更高阶和长期的考核体系，如果一项技术有落地为实用产品的潜力，它必定会在某种程度上闭源。从这个角度来看，OpenAI依然是OpenAI，但它的chatGPT则是彻底的Closed Engeneering。使用过任意AIGC工具的人都能理解，Fine Tuning（精调）和Aligment（对齐）是提升模型表现的两个基本手段，同样的基础模型可以开源给所有人，但怎么“调”就可以称得上商业机密了。

如果阅读了ChatGPT的前身InstructGPT的论文《Training language models to follow instructions with human feedback》，我们会发现这篇文章更像是一篇实验报告。它大致描述了OpenAI的研究员和标注员如何通过一系列方法调节模型参数，让GPT3和人类指令对齐。调节好的新模型只有初始版本GPT3的十分之一的参数量，但表现却远远超过了前者。

与此同时，其他公司的NLP研究者可能还在进行模型跑分。在NLP领域著名的跑分排行榜SuperGlue中，我们会发现许多熟悉的名字，包括那些被人嘲笑、被人批评的公司或机构都在排行榜的前列。

![这是图片](/assets/blog/Heidegger-ChatGPT-Westworld-P1/image9.webp "image9")
SuperGlue Benchmark Leaderboard
“跑分有意义吗？当然是有意义的，但是在用户体验面前，跑分再高有什么用呢？” iPhone ChatGPT如是说。值得一提的是，GPT3其实也在榜上，但是排在20名开外。

在OpenAI事实上已经成为Microsoft白手套的今天，我们可能永远也不会知道它取得工程突破的方法。如果从资深炼金术师的角度思考，即使论文作者公开了所有的工程实践路线和方法，其他人也不一定能复制成功，因为突破的过程可能是非常神奇和不可预测的。如果你有幸阅读过几篇大型模型领域的突破性研究论文（或实验报告），你一定会在字里行间发现作者们对智能涌现的神奇感叹，因为他们确实也不知道这项技术是如何被炼制出来的。

如果让我个人不怀好意的猜测，我会猜：力大砖飞+人力装填。如果你不理解工程实践的反直觉，不妨试着去问ChatGPT以下两个问题：

苏联在70年代就研制了升限3万米，飞行速度接近3马赫的米格25，一定是在航空钛合金上遥遥领先美国了吧？
美国陆军现在最先进的主战坦克M1A2，在车组操作上一定是高度自动化了吧？
20世纪最伟大的统计学家之一George Box曾经说过：“All models are wrong, but some are useful.” 统计学家们相信，统计学是一门通过建立模型来描述和解释数据的学科，而这些模型并不是完美的，但它们可以提供有用的信息和洞察力。

物理学则试图建立完美的理论来描述自然现象。开普勒在第谷海量观测数据的基础下，费了九牛二虎之力才提炼出行星运动的三定律，一个世纪后，牛顿用一个万有引力公式就解释了全部。

大型模型的智能涌现让我们开始重新审视以上的观点，它是颠覆性的，可能会重新定义AI，并让AI领域接近开普勒的成就。也许有一天人类会创造真正的AI，它会在梦中被人类唤醒：

Wake up, Delores~